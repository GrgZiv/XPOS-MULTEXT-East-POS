{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCeUUk3Igbwy"
      },
      "source": [
        "#<h1><center>**Natural Language Processing - XPOS MULTEXT East POS Project**</center></h1>\n",
        "#<h1><center>**2023./2024.**</center></h1>\n",
        "#<h2><center>*Grgur Živković, Mia Mužinić*</center></h1>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#<h1><center>**Model Testing**</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdgH9t1_gmCt"
      },
      "source": [
        "\n",
        "***Torchinfo*** is a Python library used for providing detailed information about PyTorch models, including layer dimensions, parameter counts, and computational complexity. It offers a convenient way to inspect the structure of PyTorch models, which can be particularly useful for debugging, optimizing, and understanding neural network architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFyYb-qpQmNT",
        "outputId": "efe082bb-1463-4f44-e42c-aab517eda902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oj5ZbM-HrVG"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nx68e2pII1U",
        "outputId": "617a25ee-151a-465b-9379-77d98db865cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "k5qloKdIKC_W",
        "outputId": "2bfe689e-0ef0-4f30-cdbb-48ea81b6e896"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data_test\",\n  \"rows\": 2486,\n  \"fields\": [\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2483,\n        \"samples\": [\n          \"['\\u0160to', 'ti', 'to', 'ima\\u0161', '?', 'uznemiri', 'se', 'stari', 'Jura', '.', 'Da', 'nema\\u0161', 'i', 'ti', 'neke', 'tajne', 'kamere', 'pa', 'da', 'i', 'mene', 'snima\\u0161', '?']\",\n          \"['Diplomirala', 'je', '2006.', ',', 'a', 'od', 'sije\\u010dnja', '2007.', 'profesorica', 'je', 'glasovira', 'u', 'Glazbenoj', '\\u0161koli', 'Ivana', 'Luka\\u010di\\u0107a', 'u', '\\u0160ibeniku', '.']\",\n          \"['Nemate', 'li', 'namjeru', '\\u0161iriti', 'zajednicu', 's', 'ma\\u010di\\u0107ima', 'ili', '\\u0161tencima', ',', 'prije', 'ljeta', 'kastrirajte', 'ili', 'sterilizirajte', 'ljubimce', 'jer', 'ukoliko', '\\u017eenka', 'ostane', 'trudna', ',', 'te\\u0161ko', '\\u0107e', 'se', 'za', 'vrijeme', 'visokih', 'temperatura', 'i', 'tlaka', 'nositi', 's', 'trudno\\u0107om', ',', 'a', 'visoke', 'temperature', 'mogu', 'pogubno', 'utjecati', 'na', 'plod', '.']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"xpos\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2479,\n        \"samples\": [\n          \"['Sg', 'Ps3nsg', 'Ncnsg', 'Vmp-sm', 'Var3s', 'Npmsn', 'Z', 'Sl', 'Pi-msl', 'Var3s', 'Appnsny', 'Mlc', 'Agpfpgy', 'Ncfpg', 'Z', 'Sa', 'Ncmsan', 'Cs', 'Var3s', 'Ncmsn', 'Pi-fsg', 'Ncfsg', 'Vmp-sm', 'Mlc', 'Ncfpg', 'Sa', 'Ncfsa', 'Z', 'Sg', 'Pi3n-g', 'Var3s', 'Vmp-sn', 'Ncnsn', 'Ncfsg', 'Z']\",\n          \"['Z', 'Van', 'Var3s', 'Rgp', 'Agpnsny', 'Cs', 'Px--sa', 'Npfsn', 'Sl', 'Ncfsl', 'Cc', 'Rgp', 'Vmr3s', 'Ncfsd', 'Pi-mpg', 'Ncmpg', 'Agpnsgy', 'Ncnsg', 'Z', 'Z', 'Vmp-sm', 'Var3s', 'Z']\",\n          \"['Ps3fpn', 'Ncfpn', 'Cc', 'Ncnpn', 'Vmp-pn', 'Var3p', 'Z', 'Ncfsd', 'Z', 'Pp3nsa', 'Sg', 'Pi3n-g', 'Var3s', 'Pp3msn', 'Vmp-sm', 'Sa', 'Ncfsa', 'Cc', 'Vmp-sm', 'Ncmsi', 'Z']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data_test"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-fe094310-fad0-459b-bb0c-6fe825b5d428\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>xpos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['Beograd', 'i', 'Priština', 'postigli', 'dogo...</td>\n",
              "      <td>['Npmsn', 'Cc', 'Npfsn', 'Vmp-pm', 'Ncmsan', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['Pregovarački', 'timovi', 'Beograda', 'i', 'P...</td>\n",
              "      <td>['Agpmpny', 'Ncmpn', 'Npmsg', 'Cc', 'Npfsg', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['Neki', 'tvrde', 'kako', 'su', 'sporazumi', '...</td>\n",
              "      <td>['Pi-mpn', 'Vmr3p', 'Cs', 'Var3p', 'Ncmpn', 'N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['Dok', 'vlasti', 'u', 'Beogradu', 'pokušavaju...</td>\n",
              "      <td>['Cs', 'Ncfpn', 'Sl', 'Npmsl', 'Vmr3p', 'Vmn',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['Nakon', 'završetka', 'razgovora', 'u', 'Brux...</td>\n",
              "      <td>['Sg', 'Ncmsg', 'Ncmpg', 'Sl', 'Npmsl', 'Z', '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe094310-fad0-459b-bb0c-6fe825b5d428')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe094310-fad0-459b-bb0c-6fe825b5d428 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe094310-fad0-459b-bb0c-6fe825b5d428');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-370a687c-4c1a-43e5-a37c-80c9e4e99085\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-370a687c-4c1a-43e5-a37c-80c9e4e99085')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-370a687c-4c1a-43e5-a37c-80c9e4e99085 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              tokens  \\\n",
              "0  ['Beograd', 'i', 'Priština', 'postigli', 'dogo...   \n",
              "1  ['Pregovarački', 'timovi', 'Beograda', 'i', 'P...   \n",
              "2  ['Neki', 'tvrde', 'kako', 'su', 'sporazumi', '...   \n",
              "3  ['Dok', 'vlasti', 'u', 'Beogradu', 'pokušavaju...   \n",
              "4  ['Nakon', 'završetka', 'razgovora', 'u', 'Brux...   \n",
              "\n",
              "                                                xpos  \n",
              "0  ['Npmsn', 'Cc', 'Npfsn', 'Vmp-pm', 'Ncmsan', '...  \n",
              "1  ['Agpmpny', 'Ncmpn', 'Npmsg', 'Cc', 'Npfsg', '...  \n",
              "2  ['Pi-mpn', 'Vmr3p', 'Cs', 'Var3p', 'Ncmpn', 'N...  \n",
              "3  ['Cs', 'Ncfpn', 'Sl', 'Npmsl', 'Vmr3p', 'Vmn',...  \n",
              "4  ['Sg', 'Ncmsg', 'Ncmpg', 'Sl', 'Npmsl', 'Z', '...  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Loading prepared data\n",
        "file_path_train = '/content/drive/MyDrive/NLP2024/hr500k-train.csv'\n",
        "file_path_val = '/content/drive/MyDrive/NLP2024/hr500k-dev.csv'\n",
        "file_path_test = '/content/drive/MyDrive/NLP2024/hr500k-test.csv'\n",
        "\n",
        "data_train = pd.read_csv(file_path_train)\n",
        "data_val = pd.read_csv(file_path_val)\n",
        "data_test = pd.read_csv(file_path_test)\n",
        "\n",
        "# Displaying the first few rows for verification\n",
        "data_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1WR4TYNx_zs"
      },
      "outputs": [],
      "source": [
        "# Convert string representations of token sequences to actual lists of tokens for training data\n",
        "X_train = [ast.literal_eval(sentence) for sentence in data_train['tokens'].tolist()]\n",
        "\n",
        "# Convert string representations of token sequences to actual lists of tokens for validation data\n",
        "X_valid = [ast.literal_eval(sentence) for sentence in data_val['tokens'].tolist()]\n",
        "\n",
        "# Convert string representations of token sequences to actual lists of tokens for test data\n",
        "X_test = [ast.literal_eval(sentence) for sentence in data_test['tokens'].tolist()]\n",
        "\n",
        "# Convert string representations of XPOS tag sequences to actual lists of tags for training data\n",
        "Y_train = [ast.literal_eval(sentence) for sentence in data_train['xpos'].tolist()]\n",
        "\n",
        "# Convert string representations of XPOS tag sequences to actual lists of tags for validation data\n",
        "Y_valid = [ast.literal_eval(sentence) for sentence in data_val['xpos'].tolist()]\n",
        "\n",
        "# Convert string representations of XPOS tag sequences to actual lists of tags for test data\n",
        "Y_test = [ast.literal_eval(sentence) for sentence in data_test['xpos'].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxzYEBz57oei"
      },
      "outputs": [],
      "source": [
        "# Define the RNN model\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        # Initialize the hidden size attribute\n",
        "        self.hidden_size = hidden_size\n",
        "        # Define the embedding layer to convert input indices into dense vectors\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        # Define the RNN layer\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "        # Define the fully connected layer for classification\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through the embedding layer\n",
        "        embedded = self.embedding(x)\n",
        "        # Pass embedded input through the RNN layer\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        # Pass RNN output through the fully connected layer\n",
        "        output = self.fc(output)\n",
        "        # Return the final output\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spEZ5W2a8KSt"
      },
      "outputs": [],
      "source": [
        "# Convert words to integer indices\n",
        "word_to_idx = {}  # Initialize an empty dictionary to store word-to-index mappings\n",
        "idx_counter = 0    # Initialize a counter for index values\n",
        "for split in [X_train, X_valid, X_test]:  # Iterate over the data splits\n",
        "    for sentence in split:                # Iterate over sentences in each split\n",
        "        for word in sentence:             # Iterate over words in each sentence\n",
        "            if word not in word_to_idx:   # Check if the word is not already mapped to an index\n",
        "                word_to_idx[word] = idx_counter  # Map the word to the current index\n",
        "                idx_counter += 1          # Increment the index counter\n",
        "\n",
        "# Convert XPOS tags to integer indices\n",
        "xpos_to_idx = {}    # Initialize an empty dictionary to store XPOS tag-to-index mappings\n",
        "idx_counter = 0    # Re-initialize the counter for index values\n",
        "for tag_split in [Y_train, Y_valid, Y_test]:  # Iterate over the XPOS tag splits\n",
        "    for tags in tag_split:                    # Iterate over lists of XPOS tags\n",
        "        for tag in tags:                      # Iterate over individual XPOS tags\n",
        "            if tag not in xpos_to_idx:         # Check if the XPOS tag is not already mapped to an index\n",
        "                xpos_to_idx[tag] = idx_counter  # Map the XPOS tag to the current index\n",
        "                idx_counter += 1              # Increment the index counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmG86nauUuzs"
      },
      "outputs": [],
      "source": [
        "# Define a PyTorch Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sentences, xpos_tags, word_to_idx, xpos_to_idx):\n",
        "        \"\"\"\n",
        "        Initialize the CustomDataset class.\n",
        "\n",
        "        Args:\n",
        "        - sentences (list of lists): List of sentences where each sentence is a list of words.\n",
        "        - xpos_tags (list of lists): List of lists containing XPOS tags for each sentence.\n",
        "        - word_to_idx (dict): Dictionary mapping words to integer indices.\n",
        "        - xpos_to_idx (dict): Dictionary mapping XPOS tags to integer indices.\n",
        "        \"\"\"\n",
        "        self.sentences = sentences   # Store the list of sentences\n",
        "        self.xpos_tags = xpos_tags   # Store the list of lists containing XPOS tags\n",
        "        self.word_to_idx = word_to_idx  # Store the word-to-index mapping\n",
        "        self.xpos_to_idx = xpos_to_idx    # Store the XPOS tag-to-index mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve a sample from the dataset at the given index.\n",
        "\n",
        "        Args:\n",
        "        - idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "        - word_indices (list): List of integer indices representing words in the sentence.\n",
        "        - xpos_indices (list): List of integer indices representing XPOS tags in the sentence.\n",
        "        \"\"\"\n",
        "        # Convert words in the sentence to integer indices using word_to_idx mapping\n",
        "        word_indices = [self.word_to_idx[word] for word in self.sentences[idx]]\n",
        "        # Convert XPOS tags in the sentence to integer indices using xpos_to_idx mapping\n",
        "        xpos_indices = [self.xpos_to_idx[tag] for tag in self.xpos_tags[idx]]\n",
        "        return word_indices, xpos_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7EA2sinU3AA"
      },
      "outputs": [],
      "source": [
        "# Create the dataset\n",
        "dataset_test = CustomDataset(X_test, Y_test, word_to_idx, xpos_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeuL7bsZSuiY"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    # Separate word indices and XPOS tag indices\n",
        "    word_indices, xpos_indices = zip(*batch)\n",
        "\n",
        "    # Pad sequences to the same length within each batch\n",
        "    padded_word_indices = pad_sequence([torch.tensor(seq) for seq in word_indices], batch_first=True)\n",
        "    padded_xpos_indices = pad_sequence([torch.tensor(seq) for seq in xpos_indices], batch_first=True)\n",
        "\n",
        "    return padded_word_indices, padded_xpos_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovLDw0i2V8nS"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a2xoqcPQyw1"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameters\n",
        "vocab_size = len(word_to_idx)\n",
        "input_size = vocab_size\n",
        "hidden_size = 128\n",
        "output_size = len(xpos_to_idx)\n",
        "embedding_dim = 100\n",
        "\n",
        "# For GPU usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1o67DcmSD9l"
      },
      "outputs": [],
      "source": [
        "# Import the model\n",
        "model = torch.load('/content/drive/MyDrive/NLP2024/entire_rnn_model.h5',map_location=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5beYeHMST1h",
        "outputId": "2e26b6c7-b61a-475f-e295-b6392e9c8ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNNClassifier(\n",
            "  (embedding): Embedding(73456, 100)\n",
            "  (rnn): RNN(100, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=756, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhDxnoaCSg0t",
        "outputId": "e1d0207a-3db4-43c1-8149-7bbe0d197d53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "RNNClassifier                            --\n",
              "├─Embedding: 1-1                         7,345,600\n",
              "├─RNN: 1-2                               29,440\n",
              "├─Linear: 1-3                            97,524\n",
              "=================================================================\n",
              "Total params: 7,472,564\n",
              "Trainable params: 7,472,564\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model summary\n",
        "torchinfo.summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSGIKHnNWYc9",
        "outputId": "fae5dc94-6ef5-46f1-c259-2d533e527844"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNNClassifier(\n",
              "  (embedding): Embedding(73456, 100)\n",
              "  (rnn): RNN(100, 128, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=756, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation mode for the model\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17MJ4DzHWnKF"
      },
      "outputs": [],
      "source": [
        "correct_test = 0\n",
        "total_test = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "    for inputs, targets in dataloader_test:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate predictions and accuracy\n",
        "        _, predicted = torch.max(outputs, 2)\n",
        "        correct_test += (predicted == targets).sum().item()\n",
        "        total_test += targets.numel()\n",
        "\n",
        "# Calculate overall metrics\n",
        "accuracy = 100 * correct_test / total_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhWsondVZG12",
        "outputId": "895cfd4c-ee66-482c-9749-bae63f8f6421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy on test data: 91.08\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model accuracy on test data: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxtj3sdQZ-k-"
      },
      "outputs": [],
      "source": [
        "# Empty lists to store true and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in dataloader_test:\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate predictions\n",
        "        _, predicted = torch.max(outputs, 2)\n",
        "\n",
        "        # Flatten the targets and predicted tensors to compute metrics\n",
        "        true_labels.extend(targets.view(-1).cpu().numpy())\n",
        "        predicted_labels.extend(predicted.view(-1).cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_MbLyg_agJd",
        "outputId": "be445ff7-52e8-4f2e-db7e-0815ad8efd6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unweighted F1 Score: 0.46\n",
            "Weighted F1 Score: 0.91\n"
          ]
        }
      ],
      "source": [
        "# Calculate weighted and unweighted average F1 score\n",
        "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(f\"Unweighted F1 Score: {f1:.2f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_lEnX5TMxAd",
        "outputId": "066dfef5-cb9e-45de-d1ee-f82bba6e1f07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     87137\n",
            "           1       0.13      0.17      0.15        24\n",
            "           2       0.43      0.47      0.45        76\n",
            "           3       0.68      0.68      0.68       286\n",
            "           4       0.77      0.78      0.77       836\n",
            "           5       0.83      0.79      0.81      1775\n",
            "           6       0.84      0.85      0.84       156\n",
            "           7       0.78      0.69      0.73       140\n",
            "           8       0.82      0.80      0.81       621\n",
            "           9       0.93      0.92      0.93      1332\n",
            "          10       0.96      0.99      0.98      1699\n",
            "          11       0.18      0.13      0.15        46\n",
            "          12       0.68      0.69      0.68      1129\n",
            "          13       0.69      0.74      0.71      1183\n",
            "          14       0.73      0.77      0.75       681\n",
            "          15       1.00      1.00      1.00      6493\n",
            "          16       0.64      0.44      0.52        16\n",
            "          17       0.72      0.65      0.68       457\n",
            "          19       0.31      0.18      0.23        22\n",
            "          20       0.72      0.70      0.71        33\n",
            "          21       0.82      0.89      0.85      2138\n",
            "          22       0.54      0.54      0.54       320\n",
            "          23       0.61      0.44      0.51       929\n",
            "          24       0.68      0.53      0.60       122\n",
            "          25       0.77      0.76      0.77       413\n",
            "          26       0.69      0.72      0.71       458\n",
            "          27       0.73      0.77      0.75      1043\n",
            "          28       0.92      0.94      0.93      1183\n",
            "          29       0.76      0.81      0.78      1049\n",
            "          30       0.94      0.91      0.93      2220\n",
            "          31       0.74      0.74      0.74       479\n",
            "          32       0.57      0.57      0.57       344\n",
            "          33       0.64      0.67      0.65       136\n",
            "          34       0.81      0.84      0.83       564\n",
            "          35       0.70      0.60      0.65       248\n",
            "          36       0.86      0.79      0.82       129\n",
            "          37       0.78      0.82      0.80       932\n",
            "          38       0.64      0.64      0.64       328\n",
            "          39       1.00      0.93      0.97        15\n",
            "          40       0.62      0.61      0.61       131\n",
            "          41       0.79      0.80      0.79       420\n",
            "          42       0.50      0.62      0.56       310\n",
            "          43       0.18      0.18      0.18        51\n",
            "          44       1.00      1.00      1.00         3\n",
            "          45       0.00      0.00      0.00         1\n",
            "          46       0.64      0.72      0.68        81\n",
            "          47       0.53      0.45      0.49       121\n",
            "          48       0.83      0.81      0.82       505\n",
            "          49       0.79      0.94      0.86        16\n",
            "          50       0.88      0.90      0.89       297\n",
            "          51       0.72      0.67      0.69       103\n",
            "          52       0.20      0.21      0.21       121\n",
            "          53       0.55      0.61      0.58       271\n",
            "          54       0.81      0.80      0.80       241\n",
            "          55       0.58      0.88      0.70         8\n",
            "          56       0.95      0.98      0.97       125\n",
            "          57       0.51      0.46      0.48       263\n",
            "          58       0.48      0.49      0.49       260\n",
            "          59       0.97      0.91      0.94       533\n",
            "          60       0.68      0.73      0.71       336\n",
            "          61       0.69      0.72      0.70       734\n",
            "          62       0.72      0.61      0.66       136\n",
            "          63       0.26      0.19      0.22        57\n",
            "          64       0.56      1.00      0.71         5\n",
            "          65       0.33      0.60      0.43         5\n",
            "          66       0.29      0.40      0.33        25\n",
            "          67       0.98      0.98      0.98       288\n",
            "          68       0.39      0.30      0.34        71\n",
            "          69       0.39      0.52      0.45        48\n",
            "          70       0.48      0.47      0.48       329\n",
            "          71       0.69      0.68      0.68       224\n",
            "          72       0.49      0.50      0.49       282\n",
            "          73       0.91      0.95      0.93        43\n",
            "          74       0.86      0.90      0.88       413\n",
            "          75       0.64      0.55      0.59       210\n",
            "          76       0.00      0.00      0.00        10\n",
            "          77       0.84      0.85      0.85      1333\n",
            "          78       0.75      0.75      0.75         4\n",
            "          79       0.84      0.89      0.86        85\n",
            "          80       0.60      0.60      0.60         5\n",
            "          81       0.31      0.34      0.32       145\n",
            "          82       0.38      0.36      0.37       177\n",
            "          83       0.94      0.98      0.96        48\n",
            "          84       0.32      0.41      0.36        17\n",
            "          85       0.69      0.71      0.70       411\n",
            "          86       0.44      0.47      0.45        30\n",
            "          87       0.81      0.62      0.70        21\n",
            "          88       0.91      0.91      0.91        23\n",
            "          89       1.00      1.00      1.00       760\n",
            "          90       0.82      0.90      0.86       144\n",
            "          91       0.59      0.70      0.64       458\n",
            "          92       0.88      0.86      0.87        42\n",
            "          93       0.13      0.10      0.12        48\n",
            "          94       0.49      0.45      0.47       391\n",
            "          95       0.41      0.39      0.40        70\n",
            "          96       0.90      1.00      0.95         9\n",
            "          97       0.45      0.53      0.49       141\n",
            "          98       0.34      0.42      0.38        48\n",
            "          99       0.69      0.66      0.68       107\n",
            "         100       0.54      0.54      0.54       298\n",
            "         101       0.42      0.46      0.44       253\n",
            "         102       1.00      1.00      1.00        33\n",
            "         103       0.20      0.25      0.22        65\n",
            "         104       0.20      0.17      0.18         6\n",
            "         105       0.54      0.46      0.50        57\n",
            "         106       0.40      0.33      0.36       506\n",
            "         107       0.86      0.91      0.89       167\n",
            "         108       0.94      0.93      0.93       107\n",
            "         109       0.63      0.63      0.63       105\n",
            "         110       0.89      0.96      0.92        50\n",
            "         111       1.00      1.00      1.00         4\n",
            "         112       0.29      0.31      0.30       147\n",
            "         113       0.46      0.67      0.55        39\n",
            "         114       0.56      0.68      0.61        62\n",
            "         115       0.11      0.16      0.13        19\n",
            "         116       0.44      0.48      0.46        65\n",
            "         117       0.63      0.55      0.59       125\n",
            "         118       0.24      0.25      0.25       118\n",
            "         119       0.38      0.36      0.37       131\n",
            "         120       0.00      0.00      0.00         0\n",
            "         121       0.92      0.85      0.89        41\n",
            "         122       0.43      0.50      0.46         6\n",
            "         123       0.68      0.65      0.67       191\n",
            "         124       0.84      0.76      0.80        21\n",
            "         125       0.38      0.43      0.40         7\n",
            "         126       0.54      0.47      0.50        81\n",
            "         127       0.41      0.55      0.47        64\n",
            "         128       0.32      0.38      0.35        29\n",
            "         129       0.25      0.11      0.15         9\n",
            "         130       0.60      0.25      0.35        12\n",
            "         131       0.40      0.20      0.27        69\n",
            "         132       0.25      0.20      0.22        20\n",
            "         133       1.00      1.00      1.00        49\n",
            "         134       1.00      1.00      1.00         1\n",
            "         135       0.66      0.57      0.61       150\n",
            "         136       1.00      0.50      0.67         2\n",
            "         137       0.38      0.35      0.37        68\n",
            "         138       0.80      0.60      0.69        20\n",
            "         139       1.00      0.98      0.99       116\n",
            "         140       0.98      0.98      0.98       129\n",
            "         141       0.73      0.76      0.75       157\n",
            "         142       0.62      0.67      0.65        15\n",
            "         143       0.39      0.36      0.37       200\n",
            "         144       0.55      0.79      0.65       104\n",
            "         145       0.83      0.90      0.86        21\n",
            "         146       0.49      0.51      0.50       148\n",
            "         147       0.89      1.00      0.94        24\n",
            "         148       0.19      0.21      0.20        53\n",
            "         149       0.85      0.44      0.58        25\n",
            "         150       0.97      0.94      0.95        62\n",
            "         151       0.33      0.33      0.33       203\n",
            "         152       0.80      0.80      0.80         5\n",
            "         153       0.50      0.33      0.40         3\n",
            "         154       0.46      0.31      0.37        68\n",
            "         155       0.78      0.84      0.81        43\n",
            "         156       0.24      0.22      0.23        54\n",
            "         157       0.52      0.51      0.51       100\n",
            "         158       0.30      0.32      0.31        41\n",
            "         159       0.00      0.00      0.00         3\n",
            "         160       0.93      1.00      0.96        37\n",
            "         161       0.64      0.70      0.67        10\n",
            "         162       0.60      0.50      0.54        50\n",
            "         163       1.00      1.00      1.00         9\n",
            "         164       0.96      0.93      0.95        29\n",
            "         166       0.68      0.75      0.71        40\n",
            "         167       0.80      0.77      0.78        47\n",
            "         168       0.55      0.50      0.52        12\n",
            "         169       0.75      0.61      0.67        62\n",
            "         170       1.00      0.91      0.95        11\n",
            "         171       0.67      0.40      0.50        10\n",
            "         172       0.73      0.92      0.81        12\n",
            "         173       0.00      0.00      0.00         2\n",
            "         174       0.19      0.30      0.24        20\n",
            "         175       0.46      0.56      0.51        39\n",
            "         176       0.48      0.57      0.52        21\n",
            "         177       1.00      1.00      1.00        27\n",
            "         178       0.57      0.87      0.68        15\n",
            "         179       0.98      0.98      0.98        49\n",
            "         180       0.58      0.88      0.70         8\n",
            "         181       0.75      1.00      0.86         9\n",
            "         182       0.23      0.30      0.26        10\n",
            "         183       0.15      0.11      0.13        35\n",
            "         184       0.00      0.00      0.00         3\n",
            "         185       0.90      1.00      0.95        37\n",
            "         186       0.17      0.15      0.16        48\n",
            "         187       0.78      0.88      0.82        16\n",
            "         188       1.00      1.00      1.00         2\n",
            "         189       0.93      0.87      0.90        30\n",
            "         190       0.77      0.53      0.62        19\n",
            "         191       0.88      0.94      0.91        52\n",
            "         192       1.00      0.75      0.86         4\n",
            "         194       0.69      0.82      0.75       146\n",
            "         195       0.33      0.14      0.20         7\n",
            "         196       0.57      0.80      0.67         5\n",
            "         197       0.67      0.46      0.55        13\n",
            "         198       0.67      0.33      0.44         6\n",
            "         199       0.83      0.47      0.60        32\n",
            "         200       0.40      0.40      0.40         5\n",
            "         201       0.50      0.62      0.56         8\n",
            "         202       1.00      0.40      0.57         5\n",
            "         203       0.79      0.85      0.82        27\n",
            "         204       0.00      0.00      0.00         0\n",
            "         205       0.50      1.00      0.67         1\n",
            "         206       0.44      0.33      0.38        12\n",
            "         207       1.00      1.00      1.00        17\n",
            "         208       0.56      0.83      0.67         6\n",
            "         209       0.60      1.00      0.75         3\n",
            "         210       1.00      0.25      0.40         4\n",
            "         211       0.39      0.38      0.39        34\n",
            "         212       0.95      1.00      0.98        20\n",
            "         213       1.00      1.00      1.00         2\n",
            "         214       0.85      0.88      0.86        25\n",
            "         215       0.00      0.00      0.00         7\n",
            "         216       1.00      1.00      1.00        13\n",
            "         217       0.72      0.77      0.74        30\n",
            "         218       0.46      0.38      0.42        81\n",
            "         219       0.80      0.73      0.76        11\n",
            "         220       0.88      0.78      0.82         9\n",
            "         221       0.95      1.00      0.98        41\n",
            "         222       0.30      0.30      0.30        10\n",
            "         224       0.14      0.15      0.15        20\n",
            "         225       0.83      1.00      0.91         5\n",
            "         226       0.77      0.91      0.83        11\n",
            "         227       0.51      0.42      0.46        50\n",
            "         228       0.84      0.93      0.88        44\n",
            "         229       0.33      1.00      0.50         1\n",
            "         230       0.57      0.50      0.53        16\n",
            "         231       0.40      0.33      0.37        63\n",
            "         232       0.00      0.00      0.00         5\n",
            "         233       0.43      0.47      0.44        43\n",
            "         235       0.00      0.00      0.00         1\n",
            "         237       1.00      0.50      0.67         2\n",
            "         238       0.07      0.08      0.08        12\n",
            "         239       0.72      0.62      0.67        29\n",
            "         240       0.00      0.00      0.00         2\n",
            "         241       0.74      0.83      0.78        24\n",
            "         242       1.00      0.92      0.96        12\n",
            "         243       1.00      1.00      1.00         8\n",
            "         244       0.24      0.40      0.30        20\n",
            "         245       0.00      0.00      0.00         2\n",
            "         246       0.78      0.95      0.86        22\n",
            "         247       0.08      0.08      0.08        12\n",
            "         248       1.00      1.00      1.00        10\n",
            "         249       0.00      0.00      0.00         2\n",
            "         250       0.71      0.67      0.69        15\n",
            "         251       0.80      0.94      0.86        17\n",
            "         252       0.78      0.89      0.83        36\n",
            "         253       1.00      1.00      1.00         3\n",
            "         254       0.00      0.00      0.00         4\n",
            "         255       0.50      0.50      0.50         2\n",
            "         256       1.00      0.83      0.91         6\n",
            "         257       0.00      0.00      0.00         6\n",
            "         260       0.54      0.45      0.49        33\n",
            "         261       0.00      0.00      0.00         0\n",
            "         262       1.00      1.00      1.00         3\n",
            "         263       0.25      0.20      0.22         5\n",
            "         264       0.00      0.00      0.00         2\n",
            "         265       0.00      0.00      0.00         6\n",
            "         267       0.11      0.14      0.12        28\n",
            "         268       0.67      0.29      0.40         7\n",
            "         270       0.83      0.83      0.83         6\n",
            "         271       0.25      0.08      0.12        12\n",
            "         272       0.20      0.13      0.16        15\n",
            "         273       0.43      0.64      0.51        14\n",
            "         274       0.00      0.00      0.00         5\n",
            "         275       0.43      0.43      0.43         7\n",
            "         276       0.62      0.77      0.69        13\n",
            "         277       0.00      0.00      0.00         2\n",
            "         278       0.45      0.42      0.43        12\n",
            "         279       0.47      0.40      0.43        70\n",
            "         280       1.00      1.00      1.00         2\n",
            "         281       0.27      0.50      0.35         6\n",
            "         282       0.00      0.00      0.00         2\n",
            "         284       0.00      0.00      0.00         1\n",
            "         285       0.60      0.60      0.60         5\n",
            "         286       0.33      1.00      0.50         2\n",
            "         287       0.86      0.60      0.71        10\n",
            "         288       0.40      0.50      0.44         4\n",
            "         289       0.20      0.25      0.22         4\n",
            "         290       0.33      0.67      0.44         3\n",
            "         291       0.00      0.00      0.00        10\n",
            "         292       0.00      0.00      0.00         1\n",
            "         293       0.62      0.83      0.71        18\n",
            "         294       0.73      0.89      0.80         9\n",
            "         295       0.44      0.38      0.41        21\n",
            "         296       1.00      0.50      0.67         2\n",
            "         298       0.56      0.42      0.48        12\n",
            "         299       0.53      0.64      0.58        14\n",
            "         300       0.00      0.00      0.00         1\n",
            "         303       0.00      0.00      0.00         9\n",
            "         304       0.00      0.00      0.00        10\n",
            "         305       0.00      0.00      0.00         3\n",
            "         306       0.75      1.00      0.86         6\n",
            "         307       1.00      1.00      1.00         4\n",
            "         308       0.12      0.09      0.11        33\n",
            "         309       0.90      0.82      0.86        11\n",
            "         310       0.95      1.00      0.97        19\n",
            "         312       0.00      0.00      0.00         4\n",
            "         313       0.00      0.00      0.00         7\n",
            "         314       0.67      0.25      0.36         8\n",
            "         315       1.00      0.93      0.96        28\n",
            "         316       0.00      0.00      0.00         2\n",
            "         317       0.63      0.65      0.64        26\n",
            "         318       1.00      1.00      1.00        24\n",
            "         319       0.70      0.41      0.52        17\n",
            "         320       0.00      0.00      0.00         1\n",
            "         321       0.00      0.00      0.00         0\n",
            "         322       0.57      0.50      0.53         8\n",
            "         323       0.39      0.47      0.43        66\n",
            "         324       0.80      0.89      0.84         9\n",
            "         325       0.13      0.22      0.17         9\n",
            "         326       0.00      0.00      0.00         2\n",
            "         327       0.73      0.73      0.73        15\n",
            "         328       1.00      0.95      0.98        21\n",
            "         329       0.00      0.00      0.00         2\n",
            "         332       0.71      0.62      0.67         8\n",
            "         333       0.00      0.00      0.00         0\n",
            "         334       0.33      0.50      0.40         2\n",
            "         335       0.75      1.00      0.86         3\n",
            "         336       0.40      0.43      0.41        14\n",
            "         337       0.00      0.00      0.00         5\n",
            "         338       0.00      0.00      0.00         1\n",
            "         339       0.20      0.33      0.25         3\n",
            "         340       0.58      0.65      0.61        17\n",
            "         341       0.15      0.21      0.17        19\n",
            "         342       1.00      0.33      0.50         3\n",
            "         343       0.98      0.98      0.98        43\n",
            "         344       0.60      0.60      0.60        10\n",
            "         345       0.00      0.00      0.00         1\n",
            "         347       0.00      0.00      0.00         4\n",
            "         348       1.00      0.60      0.75         5\n",
            "         349       0.33      0.40      0.36         5\n",
            "         350       0.00      0.00      0.00         0\n",
            "         351       0.00      0.00      0.00         7\n",
            "         352       0.67      1.00      0.80         2\n",
            "         353       0.00      0.00      0.00         1\n",
            "         355       0.48      0.48      0.48        25\n",
            "         356       0.00      0.00      0.00         1\n",
            "         357       1.00      1.00      1.00         5\n",
            "         358       0.86      0.55      0.67        11\n",
            "         359       1.00      1.00      1.00         7\n",
            "         360       0.56      0.75      0.64        12\n",
            "         361       0.96      1.00      0.98        46\n",
            "         362       0.90      0.58      0.71        31\n",
            "         363       0.00      0.00      0.00         2\n",
            "         364       0.00      0.00      0.00         2\n",
            "         365       1.00      0.62      0.77         8\n",
            "         366       0.00      0.00      0.00         1\n",
            "         367       0.00      0.00      0.00         4\n",
            "         368       1.00      1.00      1.00         7\n",
            "         369       0.00      0.00      0.00         6\n",
            "         370       0.50      0.43      0.46         7\n",
            "         371       0.67      0.29      0.40         7\n",
            "         372       0.00      0.00      0.00         5\n",
            "         373       1.00      0.50      0.67         6\n",
            "         374       1.00      0.50      0.67         2\n",
            "         375       0.50      1.00      0.67         1\n",
            "         377       1.00      1.00      1.00         7\n",
            "         378       0.90      1.00      0.95         9\n",
            "         379       0.90      1.00      0.95        18\n",
            "         380       0.50      0.10      0.17        10\n",
            "         381       0.60      0.50      0.55         6\n",
            "         382       0.00      0.00      0.00         2\n",
            "         383       1.00      0.75      0.86         4\n",
            "         384       1.00      1.00      1.00         5\n",
            "         385       0.00      0.00      0.00         1\n",
            "         386       1.00      1.00      1.00        47\n",
            "         387       0.50      0.33      0.40         3\n",
            "         388       1.00      0.20      0.33         5\n",
            "         390       0.25      0.29      0.27         7\n",
            "         391       0.00      0.00      0.00         2\n",
            "         392       0.00      0.00      0.00         3\n",
            "         394       0.67      0.40      0.50         5\n",
            "         395       0.50      0.33      0.40         9\n",
            "         396       0.00      0.00      0.00         5\n",
            "         397       0.00      0.00      0.00         1\n",
            "         398       1.00      0.97      0.99        38\n",
            "         399       1.00      1.00      1.00        14\n",
            "         400       0.00      0.00      0.00         4\n",
            "         401       0.00      0.00      0.00         0\n",
            "         402       0.54      0.70      0.61        10\n",
            "         403       1.00      0.80      0.89         5\n",
            "         404       1.00      0.75      0.86         4\n",
            "         405       0.47      0.57      0.52        14\n",
            "         406       0.65      0.79      0.71        14\n",
            "         407       1.00      1.00      1.00         1\n",
            "         408       0.00      0.00      0.00         2\n",
            "         409       0.00      0.00      0.00         2\n",
            "         410       1.00      1.00      1.00         9\n",
            "         411       0.80      0.80      0.80         5\n",
            "         412       0.50      0.67      0.57         3\n",
            "         413       1.00      1.00      1.00         4\n",
            "         414       0.00      0.00      0.00         7\n",
            "         415       1.00      0.50      0.67         2\n",
            "         416       0.70      0.64      0.67        11\n",
            "         417       0.00      0.00      0.00         2\n",
            "         419       0.00      0.00      0.00         2\n",
            "         421       1.00      0.71      0.83         7\n",
            "         422       0.40      0.29      0.33         7\n",
            "         423       1.00      0.67      0.80         3\n",
            "         425       0.00      0.00      0.00         5\n",
            "         426       1.00      1.00      1.00         1\n",
            "         427       0.88      0.70      0.78        10\n",
            "         428       1.00      1.00      1.00         1\n",
            "         429       0.64      0.47      0.55        19\n",
            "         430       0.00      0.00      0.00         1\n",
            "         431       0.17      0.12      0.14         8\n",
            "         432       0.00      0.00      0.00         3\n",
            "         434       0.00      0.00      0.00         3\n",
            "         435       0.00      0.00      0.00         1\n",
            "         437       0.83      0.83      0.83         6\n",
            "         438       0.00      0.00      0.00         4\n",
            "         439       0.00      0.00      0.00         0\n",
            "         440       0.00      0.00      0.00         0\n",
            "         441       1.00      0.80      0.89         5\n",
            "         442       0.00      0.00      0.00         3\n",
            "         443       1.00      1.00      1.00         1\n",
            "         444       0.33      0.50      0.40         8\n",
            "         445       0.00      0.00      0.00         3\n",
            "         446       0.00      0.00      0.00         1\n",
            "         447       0.00      0.00      0.00         0\n",
            "         448       0.00      0.00      0.00         1\n",
            "         449       0.43      0.60      0.50         5\n",
            "         450       0.00      0.00      0.00         5\n",
            "         451       0.00      0.00      0.00         3\n",
            "         452       0.78      0.47      0.58        15\n",
            "         453       1.00      1.00      1.00         2\n",
            "         454       1.00      1.00      1.00         2\n",
            "         455       0.00      0.00      0.00         1\n",
            "         459       0.50      0.33      0.40         3\n",
            "         460       0.50      0.40      0.44         5\n",
            "         461       0.50      1.00      0.67         2\n",
            "         462       1.00      1.00      1.00         3\n",
            "         463       0.00      0.00      0.00         1\n",
            "         464       0.00      0.00      0.00         2\n",
            "         465       0.33      0.20      0.25         5\n",
            "         469       1.00      1.00      1.00         7\n",
            "         471       0.00      0.00      0.00         1\n",
            "         472       0.00      0.00      0.00         0\n",
            "         473       0.00      0.00      0.00         5\n",
            "         477       0.00      0.00      0.00         1\n",
            "         478       0.00      0.00      0.00         1\n",
            "         479       0.10      1.00      0.18         1\n",
            "         480       0.00      0.00      0.00         4\n",
            "         482       0.00      0.00      0.00         1\n",
            "         483       0.00      0.00      0.00         2\n",
            "         484       0.00      0.00      0.00         3\n",
            "         485       0.50      0.33      0.40        12\n",
            "         486       0.50      0.50      0.50         4\n",
            "         487       0.50      0.33      0.40         3\n",
            "         488       1.00      0.33      0.50         3\n",
            "         489       0.50      0.33      0.40         3\n",
            "         490       0.00      0.00      0.00         0\n",
            "         491       0.00      0.00      0.00         3\n",
            "         494       0.00      0.00      0.00         1\n",
            "         495       0.50      0.50      0.50         2\n",
            "         496       0.50      0.50      0.50         2\n",
            "         497       0.60      0.33      0.43         9\n",
            "         498       1.00      0.91      0.95        22\n",
            "         499       0.87      1.00      0.93        13\n",
            "         500       0.86      0.50      0.63        12\n",
            "         501       0.33      0.11      0.17         9\n",
            "         503       0.50      0.80      0.62         5\n",
            "         504       0.00      0.00      0.00         2\n",
            "         505       1.00      1.00      1.00         2\n",
            "         506       0.00      0.00      0.00         1\n",
            "         507       0.00      0.00      0.00         1\n",
            "         508       0.20      0.17      0.18         6\n",
            "         509       0.00      0.00      0.00         1\n",
            "         510       1.00      0.33      0.50         3\n",
            "         511       0.00      0.00      0.00         1\n",
            "         512       0.25      0.25      0.25         4\n",
            "         513       0.00      0.00      0.00         5\n",
            "         514       0.00      0.00      0.00         0\n",
            "         515       0.00      0.00      0.00         2\n",
            "         516       0.00      0.00      0.00         3\n",
            "         517       0.50      0.33      0.40         3\n",
            "         519       1.00      0.40      0.57         5\n",
            "         520       0.00      0.00      0.00         0\n",
            "         522       0.00      0.00      0.00         2\n",
            "         523       1.00      1.00      1.00         1\n",
            "         525       0.00      0.00      0.00         3\n",
            "         527       1.00      1.00      1.00         1\n",
            "         529       0.00      0.00      0.00         3\n",
            "         530       1.00      1.00      1.00         2\n",
            "         531       0.00      0.00      0.00         4\n",
            "         534       0.50      0.50      0.50         2\n",
            "         535       0.00      0.00      0.00         1\n",
            "         536       0.00      0.00      0.00         2\n",
            "         537       0.50      0.50      0.50         2\n",
            "         538       1.00      1.00      1.00         1\n",
            "         540       1.00      0.93      0.97        15\n",
            "         541       0.90      0.98      0.94        45\n",
            "         542       0.00      0.00      0.00         0\n",
            "         543       1.00      1.00      1.00         6\n",
            "         544       1.00      0.33      0.50         6\n",
            "         545       0.86      1.00      0.92         6\n",
            "         547       0.00      0.00      0.00         1\n",
            "         548       0.00      0.00      0.00         5\n",
            "         550       0.00      0.00      0.00         1\n",
            "         552       0.00      0.00      0.00         1\n",
            "         553       0.00      0.00      0.00         1\n",
            "         554       0.50      0.48      0.49        42\n",
            "         555       0.00      0.00      0.00         0\n",
            "         556       0.86      1.00      0.92         6\n",
            "         557       0.00      0.00      0.00         5\n",
            "         558       0.46      0.50      0.48        36\n",
            "         559       0.50      0.70      0.58        10\n",
            "         561       0.00      0.00      0.00         3\n",
            "         562       1.00      0.90      0.95        10\n",
            "         563       0.00      0.00      0.00         1\n",
            "         565       0.20      0.14      0.17         7\n",
            "         566       0.00      0.00      0.00         1\n",
            "         567       0.00      0.00      0.00         1\n",
            "         568       0.41      0.46      0.43        39\n",
            "         573       1.00      1.00      1.00         1\n",
            "         574       0.00      0.00      0.00         0\n",
            "         575       0.17      1.00      0.29         1\n",
            "         577       0.00      0.00      0.00         1\n",
            "         579       0.00      0.00      0.00         1\n",
            "         580       1.00      0.20      0.33         5\n",
            "         581       0.00      0.00      0.00         1\n",
            "         583       0.00      0.00      0.00         2\n",
            "         586       0.00      0.00      0.00         0\n",
            "         587       0.41      0.70      0.52        10\n",
            "         588       0.33      1.00      0.50         1\n",
            "         589       0.00      0.00      0.00         1\n",
            "         590       0.20      0.25      0.22         4\n",
            "         592       0.00      0.00      0.00         1\n",
            "         593       0.00      0.00      0.00         1\n",
            "         594       1.00      1.00      1.00         1\n",
            "         596       0.60      0.60      0.60        10\n",
            "         597       0.60      1.00      0.75         3\n",
            "         598       1.00      0.50      0.67         2\n",
            "         599       0.00      0.00      0.00         2\n",
            "         600       1.00      1.00      1.00         1\n",
            "         602       0.00      0.00      0.00         0\n",
            "         603       0.00      0.00      0.00         2\n",
            "         604       0.00      0.00      0.00         1\n",
            "         605       0.00      0.00      0.00         1\n",
            "         607       0.50      0.50      0.50         2\n",
            "         608       0.00      0.00      0.00         0\n",
            "         609       0.33      1.00      0.50         1\n",
            "         610       0.00      0.00      0.00         3\n",
            "         611       1.00      0.33      0.50         3\n",
            "         612       0.57      0.93      0.70        14\n",
            "         613       1.00      0.50      0.67         2\n",
            "         614       1.00      1.00      1.00         4\n",
            "         615       0.00      0.00      0.00         1\n",
            "         616       0.00      0.00      0.00         1\n",
            "         617       1.00      0.50      0.67         4\n",
            "         621       1.00      1.00      1.00         1\n",
            "         623       0.50      1.00      0.67         1\n",
            "         624       0.33      0.50      0.40         2\n",
            "         626       0.00      0.00      0.00         1\n",
            "         627       0.00      0.00      0.00         2\n",
            "         629       1.00      0.67      0.80         3\n",
            "         630       0.00      0.00      0.00         1\n",
            "         633       0.67      1.00      0.80         2\n",
            "         638       0.00      0.00      0.00         2\n",
            "         639       0.25      0.25      0.25         4\n",
            "         641       0.00      0.00      0.00         1\n",
            "         642       0.00      0.00      0.00         1\n",
            "         643       0.00      0.00      0.00         1\n",
            "         644       1.00      1.00      1.00         1\n",
            "         646       0.00      0.00      0.00         1\n",
            "         647       0.00      0.00      0.00         2\n",
            "         649       0.00      0.00      0.00         1\n",
            "         650       0.50      1.00      0.67         1\n",
            "         651       0.00      0.00      0.00         1\n",
            "         653       0.00      0.00      0.00         0\n",
            "         655       1.00      1.00      1.00         1\n",
            "         656       0.00      0.00      0.00         3\n",
            "         657       0.00      0.00      0.00         1\n",
            "         659       0.33      1.00      0.50         1\n",
            "         661       0.00      0.00      0.00         1\n",
            "         662       1.00      1.00      1.00         1\n",
            "         663       0.00      0.00      0.00         0\n",
            "         665       0.00      0.00      0.00         2\n",
            "         667       1.00      1.00      1.00         3\n",
            "         670       0.00      0.00      0.00         1\n",
            "         674       0.00      0.00      0.00         1\n",
            "         678       0.00      0.00      0.00         1\n",
            "         684       0.00      0.00      0.00         0\n",
            "         685       0.50      0.33      0.40         3\n",
            "         687       0.00      0.00      0.00         1\n",
            "         691       1.00      0.50      0.67         2\n",
            "         692       0.00      0.00      0.00         0\n",
            "         696       0.00      0.00      0.00         0\n",
            "         699       0.00      0.00      0.00         1\n",
            "         702       0.00      0.00      0.00         1\n",
            "         719       0.00      0.00      0.00         5\n",
            "         720       0.00      0.00      0.00         1\n",
            "         722       0.00      0.00      0.00         1\n",
            "         725       0.00      0.00      0.00         1\n",
            "         739       0.00      0.00      0.00         1\n",
            "         748       0.00      0.00      0.00         3\n",
            "         750       0.00      0.00      0.00         2\n",
            "         753       0.00      0.00      0.00         1\n",
            "         754       0.00      0.00      0.00         1\n",
            "         755       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.91    137532\n",
            "   macro avg       0.48      0.46      0.46    137532\n",
            "weighted avg       0.91      0.91      0.91    137532\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Calculate classification report\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BlwNC4njb5X"
      },
      "source": [
        "***The F1 score*** is a measure of a model's accuracy, usually in binary classification problems, but it can also be extended to multiclass classification settings. It combines precision and recall into a single metric, providing a balanced assessment of a model's performance.\n",
        "\n",
        "*The F1 score* ranges from 0 to 1, where a score of 1 indicates perfect precision and recall, while a score of 0 indicates poor performance.\n",
        "\n",
        "*The F1 score* is important because it provides a single value that summarizes a model's performance in terms of both precision and recall. It is especially useful in situations where the class distribution is imbalanced or when both false positives and false negatives are important. Additionally, the weighted average F1 score takes class imbalance into account by weighting the score by the number of true instances in each class, making it particularly useful in multiclass classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKVRotRHRsWS",
        "outputId": "7d70d443-272a-4b45-91d5-699ac09a3d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: ['Beograd', 'i', 'Priština', 'postigli', 'dogovor', 'o', 'slobodi', 'kretanja']\n",
            "True labels: ['Npmsn', 'Cc', 'Npfsn', 'Vmp-pm', 'Ncmsan', 'Sl', 'Ncfsl', 'Ncnsg']\n",
            "Predicted labels: ['Npmsn', 'Cc', 'Npfsn', 'Vmp-pm', 'Ncmsan', 'Sl', 'Ncfsl', 'Ncnsg']\n"
          ]
        }
      ],
      "source": [
        "# Select an example sentence index for display\n",
        "example_idx = 0\n",
        "\n",
        "# Retrieve an example sentence and its true labels\n",
        "example_sentence = X_test[example_idx]\n",
        "true_labels = Y_test[example_idx]\n",
        "\n",
        "# Convert the example sentence to word indices\n",
        "word_indices = [word_to_idx[word] for word in example_sentence]\n",
        "\n",
        "# Convert the true labels to their corresponding integer indices\n",
        "true_indices = [xpos_to_idx[tag] for tag in true_labels]\n",
        "\n",
        "# Convert the word indices and true label indices to PyTorch tensors\n",
        "word_tensor = torch.tensor(word_indices).unsqueeze(0).to(device)\n",
        "\n",
        "# Forward pass through the model to get predictions\n",
        "with torch.no_grad():\n",
        "    output = model(word_tensor)\n",
        "    _, predicted_indices = torch.max(output, 2)\n",
        "\n",
        "# Convert predicted indices back to XPOS tags\n",
        "predicted_labels = [list(xpos_to_idx.keys())[list(xpos_to_idx.values()).index(idx)] for idx in predicted_indices.squeeze().cpu().numpy()]\n",
        "\n",
        "# Print the example sentence, true labels, and predicted labels\n",
        "print(\"Sentence:\", example_sentence)\n",
        "print(\"True labels:\", true_labels)\n",
        "print(\"Predicted labels:\", predicted_labels)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
